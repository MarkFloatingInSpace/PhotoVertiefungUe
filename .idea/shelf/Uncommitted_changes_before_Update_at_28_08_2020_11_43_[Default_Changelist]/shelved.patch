Index: python/buendelblock_2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: cp1252 -*-\r\n\r\n# Import the package `numpy` under the abbreviated name `np`.\r\n# NumPy provides class `ndarray` http://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html\r\n# which we use for vectors and matrices.\r\nimport numpy as np\r\n\r\n# Import `linalg` from package `scipy`.\r\n# `linalg` provides optimized algorithms for linear algebra.\r\nfrom scipy import linalg\r\n\r\n# SQLite is a data base management system.\r\n# The whole data base typically consists of a single file only.\r\n# Since it works in-memory, no separate process or even server is needed.\r\n# MonoScope stores observations in an .sqlite-file.\r\nimport sqlite3.dbapi2\r\n\r\n# h5py provides Python access to HDF5-files.\r\n# oriental.match stores feature points and their matches in this file format.\r\nimport h5py\r\n\r\n# matplotlib provides plotting functionality in Python.\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib import patheffects\r\nfrom mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\n# OpenCV\r\nimport cv2\r\n\r\n# oriental.adjust is the (bundle) adjustment package of  DLL load failed while importing cv2OrientAL.\r\nimport oriental.adjust\r\n# oriental.adjust.cost defines observation classes.\r\nimport oriental.adjust.cost\r\n# oriental.adjust.loss defines loss function classes.\r\nimport oriental.adjust.loss\r\n# oriental.adjust.local_parameterization defines local parameterizations.\r\nimport oriental.adjust.local_parameterization\r\n\r\n# Import trigonometric functions needed for the computation of residuals and derivatives.\r\n# For the computation of residuals, function arguments will be standard real numbers and hence,\r\n# the functions defined in the standard package `math` could as well be used.\r\n# However, for the computation of derivatives by automatic differentiation, custom data types\r\n# are used that only the functions defined in oriental.adjust.cost support.\r\nfrom oriental.adjust.cost import sin, cos\r\n\r\n# Extracts feature points and matches them.\r\nimport oriental.match\r\n\r\nimport oriental.ori\r\n\r\n# Provides graph algorithms.\r\nimport oriental.graph\r\n\r\n# Suppress logging.\r\nimport oriental.log\r\noriental.log.setLogFileName('')\r\n\r\nimport oriental.utils.filePaths\r\nimport os\r\nfrom pathlib import Path\r\n\r\n# The file path of the data base of image measurements done in MonoScope.\r\n# Assumes that it lies in the directory of this script, and that the default file name is used.\r\ndataBasePath = Path(os.path.abspath(__file__)).parent /'..\\data2\\MonoScope.sqlite'\r\n# dataBasePath = Path(r'..\\data\\picked_pts\\MonoScope.sqlite')\r\nprint(\"Path for the monoscope DB file: {}\".format(dataBasePath))\r\n# Make the directory of the data base file the current working directory\r\n# to ensure that the relative file paths to the image files are correct.\r\nos.chdir( dataBasePath.parent )\r\n\r\n\r\n\"\"\"returns the rotation matrix for the parameterization alpha-zeta-kappa\r\n    angular units: `gon`\r\n    equivalent to:\r\n\r\ndef alzeka2R(alzeka):\r\n    typ = type(alzeka[0])\r\n    # Trigonometric functions expect angles in radians, so convert.\r\n    alpha, zeta, kappa = alzeka * np.pi / 200.\r\n    R_alpha = np.array([ [ cos(alpha), -sin(alpha),     0.    ],\r\n                         [ sin(alpha),  cos(alpha),     0.    ],\r\n                         [     0     ,      0     ,     1.    ] ], dtype=typ )\r\n                         \r\n    R_zeta  = np.array([ [  cos(zeta),      0     , sin(zeta) ],\r\n                         [      0,          1     ,     0     ],\r\n                         [ -sin(zeta),      0     , cos(zeta) ] ], dtype=typ )\r\n                         \r\n    R_kappa = np.array([ [ cos(kappa), -sin(kappa),     0     ],\r\n                         [ sin(kappa),  cos(kappa),     0     ],\r\n                         [     0,           0,          1     ] ], dtype=typ )\r\n    # Use numpy.ndarray's operator `@` to execute matrix multiplications (instead of element-wise ones).\r\n    # Note: the object coordinate system gets rotated into the camera CS by the transpose of this rotation matrix.\r\n    return R_alpha @ R_zeta @ R_kappa\r\n\"\"\"\r\nalzeka2R=oriental.ori.alZeKaToRotationMatrix\r\n\r\n# Custom observation classes that inherit from oriental.adjust.cost.AutoDiff only need to define how to compute the\r\n# residuals, but not explicitly how to compute their derivatives, because the same definition in combination with a\r\n# special data type and automatic differentiation is used for that.\r\nclass PerspectiveCamera( oriental.adjust.cost.AutoDiff ):\r\n    \"\"\"Observation of an object point in an image of a perspective camera.\r\n    Angular units: gon\r\n    Parameterization of the rotation matrix: alpha-zeta-kappa i.e.:\r\n        1. Rotation about the z-axis (by angle alpha).\r\n        2. Rotation about the y-axis (by angle zeta).\r\n        3. Rotation about the z-axis (by angle kappa).\r\n    \"\"\"\r\n\r\n    def __init__( self, x_observed, y_observed, ptName=None, phoName=None ):\r\n        \"\"\"Initialize an instance of class `PerspectiveCamera` with the passed image point coordinates.\"\"\"\r\n\r\n        # Store the image coordinates as member variables of this instance of `PerspectiveCamera`.\r\n        self.x_observed = x_observed\r\n        self.y_observed = y_observed\r\n        self.ptName = ptName\r\n        self.phoName = phoName\r\n\r\n        # Define the number of residuals that method `Evaluate` computes.\r\n        # In the 2-dimensional image coordinate system, these are 2 residuals per image point.\r\n        numberOfResiduals = 2\r\n\r\n        # The number of elements of `parameterBlockSizes` defines the number of parameter blocks that method `Evaluate`\r\n        # expects. Each element of `parameterBlockSizes` defines the number of elements that method `Evaluate` expects\r\n        # the respective parameter block to have.\r\n        # The number and sizes of parameter blocks defined here must match the number and sizes of parameter blocks\r\n        # passed to block.AddResidualBlock(...).\r\n        parameterBlockSizes = (\r\n            3, # Projection center in the object coordinate system.\r\n            3, # Rotation angles.\r\n            3, # Interior orientation i.e. projection center in the camera coordinate system.\r\n            5, # TODO Lens distortion parameters, not (yet) used here. Adapt this number to your distortion model.\r\n            3  # Object point.\r\n        )\r\n        # Hence, `PerspectiveCamera` expects 5 parameter blocks, where each block consists of 3 elements\r\n        # (i.e. unknowns), except for the block of distortion parameters.\r\n\r\n        # Initialize the base class.\r\n        super().__init__( numberOfResiduals, parameterBlockSizes )\r\n\r\n    def Evaluate( self, parameterBlocks, residuals ):\r\n        \"\"\"returns the residuals of a single image point observation, based on the current values of unknown parameters.\r\n\r\n        Reads `parameterBlocks`, and stores the computed residuals in `residuals`.\r\n\r\n        `parameterBlocks` is a list of parameter blocks. Its length has been defined above as the length of `parameterBlockSizes`.\r\n        Each element of `parameterBlocks` is a parameter block.\r\n        Each parameter block is a numpy.ndarray of 1 dimension (i.e. a vector).\r\n        Each parameter block contains as many elements as specified above by the respective element of `parameterBlockSizes`.\r\n        Each element is of the current value of the respective unknown parameter, which generally changes with each iteration.\r\n\r\n        `residuals` contains as many elements as specified above by `numberOfResiduals`.\r\n        `Evaluate` is expected to set each element of `residuals`.\r\n        \"\"\"\r\n\r\n        # Basically, `Evaluate` gets called for `numpy.ndarray`s of real numbers, to compute residuals based on the\r\n        # current values of unknowns.\r\n        # However, `Evaluate` also gets called for `numpy.ndarray`s whose elements are of a special data type\r\n        # (oriental.adjust.cost.Jet), to compute the rows of the model matrix that correspond to this image observation.\r\n        # This detail needs to be considered only when `Evaluate` itself brings real numbers into play -\r\n        # for `PerspectiveCamera`, this is the case with the observed image coordinates.\r\n        # For this purpose, store the data type of an arbitrary parameter.\r\n        typ = type(parameterBlocks[0][0])\r\n\r\n        \"\"\" Read the parameter blocks. \"\"\"\r\n        prc    = parameterBlocks[0] # Projection center.\r\n\r\n        alzeka = parameterBlocks[1] # Rotation angles [gon], rotation matrix parameterization alpha-zeta-kappa.\r\n\r\n        ior    = parameterBlocks[2] # Interior orientation.\r\n\r\n        # A parameter block for distortion parameters is already defined here, but not (yet) used.\r\n        # TODO Choose an appropriate distortion model,\r\n        #      adapt the expected size of that parameter block in parameterBlockSizes above,\r\n        #      and actually use that parameter block here in the computation of residuals.\r\n        distortion = parameterBlocks[3]\r\n\r\n        objPt = parameterBlocks[4]  # Object point.\r\n\r\n\r\n        # Elements of the interior orientation.\r\n        x_0 = ior[0] + typ(0.) # Principal point's x-coordinate.\r\n        y_0 = ior[1] + typ(0.) # Principal point's y-coordinate.\r\n        c   = ior[2]           # Focal length.\r\n\r\n        # TODO implement a distortion model and apply it.\r\n\r\n        \"\"\" Project the object point into the image and compute the residuals. \"\"\"\r\n\r\n        # Reduce the object point coordinates to the projection center in the object coordinate system.\r\n        objPtRed = objPt - prc\r\n\r\n        # Compute the rotation matrix using the passed rotation angles, considering the parameterization in use.\r\n        R = alzeka2R(alzeka)\r\n\r\n        # Rotate the reduced object points into the camera coordinate system.\r\n        # Mind that for this purpose, the transpose of the rotation matrix is used by convention.\r\n        camPt = R.T.dot(objPtRed)\r\n\r\n        # Project the reduced, rotated point into the image plane.\r\n        x_projected = x_0 - c * camPt[0] / camPt[2]\r\n        y_projected = y_0 - c * camPt[1] / camPt[2]\r\n\r\n        # Markus: calculate distortion correction:\r\n        x_ = x_projected - x_0\r\n        y_ = y_projected - y_0\r\n        rho = (x_**2 + y_**2) ** (1/2)\r\n\r\n        # sensor size 6000 x 4000\r\n        rho0 = (3000 ** 2 + 2000 ** 2) ** (1/2)\r\n\r\n        k1 = distortion[0]\r\n        k2 = distortion[1]\r\n        k3 = distortion[2]\r\n        p1 = distortion[3]\r\n        p2 = distortion[4]\r\n\r\n        delta_x_dist = x_*(k1*(rho**2-rho0**2) + k2 * (rho**4-rho0**4) + k3 * (rho**6-rho0**6)) + \\\r\n                       2 * p1 * x_ * y_ + p2 * (rho ** 2 + 2 * x_ ** 2)\r\n        delta_y_dist = y_*(k1*(rho**2-rho0**2) + k2 * (rho**4-rho0**4) + k3 * (rho**6-rho0**6))+ \\\r\n                       + p1 * (rho ** 2 + 2 * y_ ** 2) + 2 * p2 * x_ * y_\r\n\r\n        #Radiale und tangeniale Verzeichnung beides 3. und 5. Ordnung\r\n        # delta_x_dist = x_*(k1*(rho**2-5000000) + k2 * (rho ** 4-5000000)) + 2 * p1 * x_ * y_ + p2 * (rho ** 2 + 2 * x_ ** 2)\r\n        # delta_y_dist = y_*(k1*(rho**2-5000000) + k2 * (rho ** 4-5000000)) + p1 * (rho ** 2 + 2 * y_ ** 2) + 2 * p2 * x_ * y_\r\n\r\n        # delta_x_dist = x_ * (k1 * rho ** 2 + k2 * rho ** 4) + 2 * p1 * x_ * y_ + p2 * (rho ** 2 + 2 * x_ ** 2)\r\n        # delta_y_dist = y_ * (k1 * rho ** 2 + k2 * rho ** 4) + 2 * p2 * x_ * y_ + p1 * (rho ** 2 + 2 * y_ ** 2)\r\n\r\n\r\n        x_projected += delta_x_dist\r\n        y_projected += delta_y_dist\r\n\r\n        # Compute the residuals.\r\n        residuals[0] = self.x_observed - x_projected\r\n        residuals[1] = self.y_observed - y_projected\r\n\r\n\r\n        # Return True to indicate a successful computation.\r\n        return True\r\n\r\n\r\ndef loadImageObservations():\r\n    \"\"\"returns the images points observed in MonoScope, ordered by photos.\r\n\r\n    MonoScope creates a data base consisting of 2 tables:\r\n    1. Table `images` lists all photos, with columns:\r\n         `id`  : image serial number,\r\n         `path`: image file path, relative to the data base file path.\r\n\r\n    2. Table `imgobs` stores on each row an image point observation, with columns:\r\n         `id`    : image observation serial number,\r\n         `imgid` : serial number of image that corresponds to this observation,\r\n         `name`  : point name,\r\n         `x`     : x-coordinate in the camera coordinate system, and\r\n         `y`     : y-coordinate in the camera coordinate system.\r\n    The data base can be inspected and edited with standard tools,\r\n    e.g. using $ORIENTAL_ROOT/shared/spatialite_gui.exe,\r\n    or Python's package `sqlite3`, as done here.\r\n    Use SQL for data base queries.\"\"\"\r\n\r\n    # Open a read-only connection to dataBasePaththe data base that has been created with MonoScope.\r\n    with sqlite3.dbapi2.connect(dataBasePath.as_uri() + '?mode=ro', uri=True) as connection:\r\n        connection.row_factory = sqlite3.dbapi2.Row\r\n\r\n        # Create a `dict` (associative array) that maps image file paths to the corresponding image point observations.\r\n        photo_obs = dict()\r\n\r\n        # Get the serial numbers and file paths of all images.\r\n        photoRows = connection.execute(\"\"\"\r\n            SELECT id, path\r\n            FROM images\r\n        \"\"\")\r\n\r\n        # Iterate over all result rows of the query above.\r\n        for photoRow in photoRows:\r\n\r\n            # Create a `dict` for the current photo that maps for each of its point observations\r\n            # the point name (as key) to a numpy.ndarray that contains the resp. image coordinates.\r\n            ptNames_coord = dict()\r\n\r\n            # Get all image point observations for the current photo.\r\n            ptRows = connection.execute(\"\"\"\r\n                SELECT name, x, y\r\n                FROM imgobs\r\n                WHERE imgid=?\r\n            \"\"\", [photoRow['id']] )\r\n\r\n            # Iterate over all result rows of the previous query.\r\n            for ptRow in ptRows:\r\n                ptNames_coord[ptRow['name']] = np.array([ptRow['x'],\r\n                                                         ptRow['y']])\r\n\r\n            # Insert the `dict` of image point observations in the current photo\r\n            # into photo_obs, with the image file path as key.\r\n            photo_obs[photoRow['path']] = ptNames_coord\r\n\r\n    return photo_obs\r\n\r\ndef spatialResection(photo_obs, ior, objPts):\r\n    \"\"\"Spatial resection\"\"\"\r\n    projectionCenters = {}\r\n    rotationAngles = {}\r\n    for photo, observations in photo_obs.items():\r\n        pts = []\r\n        for ptName, imageCoordinates in observations.items():\r\n            objPt = objPts.get(ptName)\r\n            if objPt is not None:\r\n                pts.append((*( imageCoordinates * (1,-1) ), *(objPt*(1,-1,-1))))\r\n        if len(pts) < 4:\r\n            raise Exception(f'At least 4 corresponding image and object points are needed for an unambiguous spatial resection, but only {len(pts)} are present.')\r\n        pts=np.array(pts)\r\n        K=oriental.ori.cameraMatrix(ior)\r\n        objectPoints = pts[:, 2:].reshape((-1, 1, 3)).copy()\r\n        imagePoints = pts[:, :2].reshape((-1, 1, 2)).copy()\r\n        rvec, tvec = cv2.solvePnP(objectPoints=objectPoints,\r\n                                  imagePoints =imagePoints,\r\n                                  cameraMatrix=K,\r\n                                  distCoeffs=np.empty(0),\r\n                                  flags=cv2.SOLVEPNP_EPNP)[1:]\r\n        rvec, tvec = cv2.solvePnP(objectPoints=objectPoints,\r\n                                  imagePoints =imagePoints,\r\n                                  cameraMatrix=K,\r\n                                  distCoeffs=np.empty(0),\r\n                                  rvec=rvec,\r\n                                  tvec=tvec,\r\n                                  useExtrinsicGuess=True,\r\n                                  flags=cv2.SOLVEPNP_ITERATIVE)[1:]\r\n        res = pts[:, :2] - cv2.projectPoints(pts[:, 2:].reshape((-1, 1, 3)).copy(), rvec, tvec, K, np.empty(0))[0].squeeze()\r\n        print('Photo {} max res norm: {:.2f}px'.format(photo, np.sum(res**2,axis=0).max()**.5))\r\n\r\n        R, t = oriental.ori.projectionMat2oriRotTrans(np.column_stack((cv2.Rodrigues(rvec)[0], tvec)))\r\n        projectionCenters[photo] = t\r\n        rotationAngles[photo] = oriental.ori.rotationMatrixToAlZeKa(R.copy())\r\n\r\n    return projectionCenters, rotationAngles\r\n\r\n\r\ndef getAutomaticTiePoints(imageFilePaths):\r\n    \"\"\"Automatically compute additional tie points.\r\n    Tie points and their matches do not depend on exterior image orientations, and their computation is elaborate.\r\n    Hence, re-use existing ones, if available.\"\"\"\r\n    # oriental.match.match stores its results in the following file:\r\n    hdf5Path = dataBasePath.parent / 'features.h5'\r\n    points = dict()\r\n    matches = dict()\r\n    if not hdf5Path.exists():\r\n        print(\"Extract new feature points and match them. This will take some time.\")\r\n        detectionOptions = oriental.match.SiftOptions()\r\n        detectionOptions.nAffineTilts = 0#6\r\n        filterOptions = oriental.match.FeatureFiltOpts()\r\n        filterOptions.nRowCol = 3, 4\r\n        matchingOptions = oriental.match.MatchingOpts()\r\n        oriental.match.match(imagePaths = imageFilePaths,\r\n                             featureDetectOpts = detectionOptions,\r\n                             featureFiltOpts = filterOptions,\r\n                             matchingOpts = matchingOptions,\r\n                             outDir = str(hdf5Path.parent))\r\n    else:\r\n        print(f\"Load existing feature points and their matches from file '{hdf5Path}'\")\r\n\r\n    with h5py.File(hdf5Path, 'r') as hdf5File:\r\n        imageName2Path = {Path(path).name : path for path in imageFilePaths}\r\n        for imageName, hdf5Points in hdf5File['keypts'].items():\r\n            imagePath = imageName2Path[imageName]\r\n            points[imagePath] = np.array(hdf5Points[:, :2]).astype(float)\r\n        for imagePairNames, hdf5Matches in hdf5File['matches'].items():\r\n            imagePairPaths = tuple(imageName2Path[imageName] for imageName in imagePairNames.split('?'))\r\n            matches[imagePairPaths] = np.array(hdf5Matches)\r\n\r\n    return points, matches\r\n\r\ndef linkAutomaticTiePoints(matches):\r\n    \"\"\"Link automatic tie points that correspond to the same object point.\r\n    This is important for object points that have been observed in more than only 2 images, because otherwise, an\r\n    independent object point would be adjusted for each image pair, which would reduce redundancy.\r\n    \"\"\"\r\n    imageFilePaths = {el for imagePairPaths in matches for el in imagePairPaths}\r\n    imagePath2Index = {path : idx for idx, path in enumerate(imageFilePaths)}\r\n\r\n    pointChains = oriental.graph.ImageFeatureTracks()\r\n    for imagePairPaths, matchesImagePair in matches.items():\r\n        imageIndex1 = imagePath2Index[imagePairPaths[0]]\r\n        imageIndex2 = imagePath2Index[imagePairPaths[1]]\r\n        for iPt1, iPt2 in matchesImagePair:\r\n            feature1 = oriental.graph.ImageFeatureID(imageIndex1, int(iPt1))\r\n            feature2 = oriental.graph.ImageFeatureID(imageIndex2, int(iPt2))\r\n            pointChains.join(feature1, feature2)\r\n    pointChains.compute()\r\n    components = pointChains.components()\r\n\r\n    index2ImagePath = {idx : path for path, idx in imagePath2Index.items()}\r\n    autoImageObs = []\r\n    for features in components.values():\r\n        if len({feature.iImage for feature in features}) < len(features):\r\n            continue # drop feature point chains with multiple projections into the same image\r\n        imgPtObs = []\r\n        for feature in features:\r\n            imagePath = index2ImagePath[feature.iImage]\r\n            imagePtIndex = int(feature.iFeature)\r\n            imgPtObs.append((imagePath, imagePtIndex))\r\n        autoImageObs.append(imgPtObs)\r\n    return autoImageObs\r\n\r\ndef thinOutTracks( featurePoints, autoImageObs, nCols, nRows, minFeaturesPerCell ):\r\n    print( f'Thin out feature tracks on {nCols}x{nRows} (cols x rows) grids, keeping at least {minFeaturesPerCell} features per cell with largest multiplicity' )\r\n    imgRowsCols = 4912, 7360 # image resolution of D800\r\n    def getRowCol( xy ):\r\n        col = int(  xy[0] / ( imgRowsCols[1] / nCols ) )\r\n        row = int( -xy[1] / ( imgRowsCols[0] / nRows ) )\r\n        return row, col\r\n\r\n    thinnedAutoImageObs = []\r\n    featureCounts = { phoPath : np.zeros( imgRowsCols, int ) for phoPath in featurePoints }\r\n    autoImageObs.sort( key=lambda x: len(x), reverse=True )\r\n    for phoPathPtIds in autoImageObs:\r\n        for phoPath, ptId in phoPathPtIds:\r\n            xy = featurePoints[phoPath][ptId]\r\n            row, col = getRowCol(xy)\r\n            if featureCounts[phoPath][row,col] < minFeaturesPerCell:\r\n                break\r\n        else:\r\n            continue\r\n        for phoPath, ptId in phoPathPtIds:\r\n            xy = featurePoints[phoPath][ptId]\r\n            row, col = getRowCol(xy)\r\n            featureCount = featureCounts[phoPath]\r\n            featureCount[row,col] += 1\r\n        thinnedAutoImageObs.append(phoPathPtIds)\r\n    print( 'Thinned out features per image\\n'\r\n           'img\\t#features\\t#fullCells\\n'\r\n           '{}'.format(\r\n           '\\n'.join( '{}\\t{}\\t{}'.format( phoPath, counts.sum(), np.sum( counts >= minFeaturesPerCell ) )\r\n                                           for phoPath, counts in featureCounts.items() ) ) )\r\n    return thinnedAutoImageObs\r\n\r\ndef forwardIntersectionLinear( observations, projectionCenters, rotationAngles, ior, compResiduals=False ):\r\n    \"\"\"forward intersects image observations and returns object point coordinates that yield a minimum sum of\r\n     squared algebraic residuals.\"\"\"\r\n\r\n    # Kinv is the inverted camera matrix:\r\n    # Kinv @ homogeneousImageCoordinates yields image coordinates reduced to the principal point,\r\n    # for a camera of unit focal length.\r\n    x0, y0, c = ior\r\n    Kinv = np.array([[-1./c,    0., x0/c],\r\n                     [   0., -1./c, y0/c],\r\n                     [   0.,    0.,   1.]])\r\n\r\n    A = np.zeros(( 2*len(observations), 4 ))\r\n\r\n    for idx,(photo,imageCoordinates) in enumerate(observations):\r\n        imageCoordinatesNormed = Kinv @ np.r_[ imageCoordinates, 1 ]\r\n        R = alzeka2R( rotationAngles[photo] )\r\n        P = np.c_[ R.T, -R.T @ projectionCenters[photo] ]\r\n        A[idx*2  ] = imageCoordinatesNormed[0]*P[2,:] - P[0,:]\r\n        A[idx*2+1] = imageCoordinatesNormed[1]*P[2,:] - P[1,:]\r\n\r\n    U, s, Vt = linalg.svd(A)\r\n    X = Vt.T[:,-1]\r\n    X = X[:3] / X[3]\r\n\r\n    if compResiduals:\r\n        K = np.array([[ -c,  0, x0 ],\r\n                      [  0, -c, y0 ],\r\n                      [  0,  0, 1. ]])\r\n        residuals = np.zeros((len(observations),2))\r\n        for idx,(photo,imageCoordinates) in enumerate(observations):\r\n            R = alzeka2R( rotationAngles[photo] )\r\n            P = np.c_[ R.T, -R.T @ projectionCenters[photo] ]\r\n            camSys = P @ np.r_[ X, 1 ]\r\n            if camSys[2] >= 0:\r\n                return X, np.ones(2) * np.inf # X is behind this camera\r\n            projection = K @ camSys\r\n            projection = projection[:2] / projection[2]\r\n            residuals[idx,:] = imageCoordinates - projection\r\n        return X, residuals\r\n\r\n    return X\r\n\r\ndef forwardIntersectionGeometric( observations, projectionCenters, rotationAngles, ior, distortion, maxLinResNorm=None ):\r\n    \"\"\"forward intersects image observations and returns object point coordinates that yield a minimum sum of\r\n     squared geometric residuals, together with the residuals themselves.\r\n    Returns infinite residuals in case of a rank deficit.\r\n    \"\"\"\r\n    linRes = forwardIntersectionLinear( observations, projectionCenters, rotationAngles, ior, maxLinResNorm is not None )\r\n    if maxLinResNorm is not None:\r\n        X, residualsLin = linRes\r\n        if not np.isfinite(residualsLin).all():\r\n            return X, np.ones(2)*np.inf\r\n        if np.sum(residualsLin ** 2, axis=1).max() > maxLinResNorm ** 2:\r\n            return X, np.ones(2)*np.inf\r\n    else:\r\n        X = linRes\r\n\r\n    loss = oriental.adjust.loss.Trivial()\r\n    block = oriental.adjust.Problem()\r\n    for photo, imageCoordinates in observations:\r\n        x, y = imageCoordinates\r\n        cost = PerspectiveCamera( x, y )\r\n        block.AddResidualBlock( cost,\r\n                                loss,\r\n                                projectionCenters[photo],\r\n                                rotationAngles[photo],\r\n                                ior,\r\n                                distortion,\r\n                                X )\r\n    evaluateOptions = oriental.adjust.Problem.EvaluateOptions()\r\n    # Query only columns of the A-matrix that correspond to the object point coordinates.\r\n    evaluateOptions.set_parameter_blocks( [ X ] )\r\n    try:\r\n        for iIter in range(10):\r\n            l, A = block.Evaluate( evaluateOptions, residuals=True, jacobian=True )\r\n            A = -A.toarray()\r\n            Atl = A.T @ l\r\n            N = A.T @ A\r\n            C = linalg.cholesky( N, lower=False )\r\n            delta_x = linalg.cho_solve( (C,False), Atl )\r\n            X += delta_x\r\n            # This is not a generally applicable stopping criterion. But at this point, it should be good enough.\r\n            if np.abs(delta_x).max() < 1.e-3:\r\n                break\r\n    except linalg.LinAlgError:\r\n        return X, np.ones(2)*np.inf\r\n\r\n    l, = block.Evaluate( residuals=True )\r\n    return X, l.reshape((-1,2))\r\n\r\n\r\ndef initializeObjectCoordinates( photo_obs, projectionCenters, rotationAngles, ior, distortion, objCoordinates ):\r\n    \"\"\"initialize the object point coordinates\"\"\"\r\n\r\n    # For each point name, collect all corresponding image points.\r\n    # For that purpose, create a `dict`\r\n    # - with point names as keys and\r\n    # - lists of image names and corresponding image point coordinates as values.\r\n    ptNames_obs = dict()\r\n\r\n    for photo, observations in photo_obs.items():\r\n        for ptName, imageCoordinates in observations.items():\r\n            ptNames_obs.setdefault( ptName, [] ).append( ( photo, imageCoordinates ) )\r\n\r\n    # Determine for each non-datum point its object coordinates.\r\n    for ptName, observations in ptNames_obs.items():\r\n\r\n        if ptName in objCoordinates:\r\n            # Object point is a datum point.\r\n            continue\r\n\r\n        if len(observations) == 0:\r\n            raise Exception(f\"Internal error: point {ptName} has not been observed in any image!\")\r\n\r\n        if len(observations) == 1:\r\n            raise Exception(f\"Point {ptName} has been observed only once (in image {observations[0][0]}), \"\r\n                             \"and hence cannot be initialized\")\r\n\r\n        # Initialize the object point coordinates.\r\n        objCoordinates[ptName], _ = forwardIntersectionGeometric( observations, projectionCenters, rotationAngles, ior, distortion )\r\n\r\n\r\ndef printParameters( projectionCenters, rotationAngles, ior, distortion, objPts ):\r\n    def printArr(name, arr, single=False):\r\n        print(\"r'{}' : np.array([{}]){}\".format(name, ', '.join(f'{el:+8.5e}' for el in arr), '' if single else ','))\r\n\r\n    printArr( 'ior', ior, single=True )\r\n\r\n    printArr('distortion', distortion, single=True)\r\n\r\n    print(\"projection centers:\")\r\n    for name, prjCtr in sorted( projectionCenters.items(), key=lambda x: x[0] ):\r\n        printArr( name, prjCtr )\r\n\r\n    print(\"rotation angles:\")\r\n    for name, rotation in sorted( rotationAngles.items(), key=lambda x: x[0] ):\r\n        printArr( name, rotation )\r\n\r\n    nAutoTiePoints=0\r\n    print(\"object points:\")\r\n    for name, objPt in sorted(objPts.items(), key=lambda x: sortPoints(x[0])):\r\n        if name.startswith('a'):\r\n            nAutoTiePoints += 1\r\n            continue\r\n        printArr(name, objPt)\r\n    if nAutoTiePoints:\r\n        print(f'{nAutoTiePoints} automatic tie points not printed')\r\n\r\n\r\ndef residualStatistics( block, residuals, k=10 ):\r\n    squaredResidualNorms = residuals[0::2]**2 + residuals[1::2]**2\r\n    k = min(k,len(squaredResidualNorms))\r\n    idxLargestResidualsNorms = np.argsort(squaredResidualNorms)[::-1]\r\n    residualBlocks = block.GetResidualBlocks()\r\n    print(f'The {k} largest residual norms:\\n'\r\n           'photo\\tPoint name\\tResidual norm')\r\n    for iResidualNorm in idxLargestResidualsNorms[:k]:\r\n        residualBlock = residualBlocks[iResidualNorm]\r\n        cost = block.GetCostFunctionForResidualBlock(residualBlock)\r\n        residualNorm = squaredResidualNorms[iResidualNorm]**.5\r\n        print(f'{cost.phoName}\\t{cost.ptName}\\t{residualNorm:.2f}')\r\n\r\n\r\ndef residualHistogram( residuals, suffix, show ):\r\n    tit = \"Histogram of Residuals \" + suffix\r\n    fig=plt.figure(tit)\r\n    maxAbs = np.abs( residuals ).max()\r\n    plt.hist( residuals, bins=11, range=(-maxAbs,maxAbs) )\r\n    plt.ylabel('Absolute Frequency')\r\n    plt.xlabel('Residual [px]')\r\n    plt.title(tit)\r\n    plotDir = Path('residuals ' + suffix)\r\n    plt.savefig(plotDir / 'residualHistogram.png')\r\n    if show:\r\n        plt.show(block=False)\r\n    else:\r\n        plt.close(fig)\r\n\r\n\r\ndef residualPlots( block, residuals, ior, suffix, show, scale=1. ):\r\n    plotDir = Path('residuals ' + suffix)\r\n    print(f'Saving plots to directory \"{plotDir}\"')\r\n    os.makedirs(plotDir, exist_ok=True)\r\n    dpi = 200\r\n    def plot(phoObsResidsNames, radius):\r\n        plt.scatter(ior[0], ior[1], s=40, c='r', marker='+')\r\n        plt.plot(ior[0] + radius * np.cos(np.linspace(0, 2*np.pi, 400)),\r\n                 ior[1] + radius * np.sin(np.linspace(0, 2*np.pi, 400)), 'r')\r\n        # Plot automatic points first, such that they cannot hide manual ones.\r\n        sz=3\r\n        # Plot automatic tie points in magenta, and manual ones in cyan.\r\n        colors = np.array(['m' if ptName.startswith('a') else 'c' for (*_, ptName) in phoObsResidsNames])\r\n\r\n        phoObsResids = np.array([el[:4] for el in phoObsResidsNames])\r\n        plt.scatter(x=phoObsResids[:,0], y=phoObsResids[:,1], s=sz, marker='o', edgecolors='k', facecolors='k')\r\n        lines=[]\r\n        for color in 'm', 'c':\r\n            act = colors == color\r\n            if not act.any():\r\n                continue\r\n            lines.extend(plt.plot(np.column_stack((phoObsResids[act,0], phoObsResids[act,0] - phoObsResids[act,2] * scale)).T,\r\n                                  np.column_stack((phoObsResids[act,1], phoObsResids[act,1] - phoObsResids[act,3] * scale)).T,\r\n                                  color=color, linewidth=sz**.5 ))\r\n            lines[-1].set_label('auto' if color=='m' else 'manu')\r\n        # Representative residual norm, to be shown graphically. Cast to int for better readability.\r\n        repResNorm = np.ceil(np.percentile(np.sum(phoObsResids[:, 2:] ** 2, axis=1), 90) ** .5).astype(int)\r\n        ax = plt.gca()\r\n        ax.add_artist(AnchoredSizeBar(ax.transData, size=scale * repResNorm, label=f'{repResNorm}px', loc=4, color='c'))\r\n        plt.xlabel('x')\r\n        plt.ylabel('y')\r\n        return lines, ( phoObsResids[:,2:]**2 ).sum(axis=1).max() **.5\r\n\r\n    photo_obsResidsNames = {}\r\n    for iResidual, residualBlock in enumerate(block.GetResidualBlocks()):\r\n        cost = block.GetCostFunctionForResidualBlock(residualBlock)\r\n        photo_obsResidsNames.setdefault(cost.phoName, []).append(\r\n            (cost.x_observed, cost.y_observed,\r\n             residuals[iResidual*2], residuals[iResidual*2+1],\r\n             cost.ptName))\r\n\r\n    maxResidNorm = 0.\r\n    for imagePath, obsResidsNames in photo_obsResidsNames.items():\r\n        image = plt.imread(imagePath)\r\n        lowResScale = float(1000 / max(image.shape[:2]))\r\n        imgLowRes = cv2.resize(image, (0, 0), fx=lowResScale, fy=lowResScale , interpolation=cv2.INTER_AREA) # down-sample to speedup plotting\r\n        fig=plt.figure(imagePath + ' Residuals ' + suffix, clear=True, constrained_layout=True, dpi=dpi)\r\n        plt.imshow(imgLowRes, interpolation='nearest', cmap='gray', extent=(-.5, image.shape[1]-.5, -image.shape[0]+.5, .5))\r\n        plt.autoscale(False)\r\n\r\n        lines, maxResidNorm_ = plot(obsResidsNames, radius=linalg.norm(image.shape[:2])/3)\r\n        for x, y, resX, resY, ptName in obsResidsNames:\r\n            if not ptName.startswith('a'):\r\n                txt = plt.text(x, y, ptName, color='k', size='small', zorder=1,\r\n                               clip_on=True, path_effects=[patheffects.withStroke(linewidth=1, foreground='white')])\r\n        plt.title(f\"Residuals {suffix}, {scale} times enlarged. Max: {maxResidNorm_:.1f}px\")\r\n        maxResidNorm = max(maxResidNorm, maxResidNorm_)\r\n\r\n        plt.legend(loc='best')\r\n        plt.savefig(plotDir / ( Path(imagePath).stem + '.png' ))\r\n        if not show:\r\n            plt.close(fig)\r\n\r\n    fig=plt.figure('All Residuals ' + suffix, clear=True, constrained_layout=True, dpi=dpi)\r\n    plot([el for obsResidsNames in photo_obsResidsNames.values() for el in obsResidsNames], radius=linalg.norm(image.shape[:2])/3)\r\n    plt.axis('image')\r\n    plt.xlim((-.5, image.shape[1]-.5))\r\n    plt.ylim((-image.shape[0]+.5, .5))\r\n    plt.title(f'All Residuals {suffix}, {scale} times enlarged. Max: {maxResidNorm:.1f}px')\r\n    plt.savefig(plotDir / 'all.png')\r\n    print('Plotting finished')\r\n    if show:\r\n        plt.show(block=False)\r\n    else:\r\n        plt.close(fig)\r\n\r\n    residualHistogram(residuals, suffix, show)\r\n\r\ndef sortPoints( arg ):\r\n    try:\r\n        return int(arg)\r\n    except ValueError:\r\n        try:\r\n            return 1.e6 + int(arg[1:])\r\n        except ValueError:\r\n            return 1.e7\r\n\r\ndef plot3d( titleSuffix, projectionCenters, rotationAngles, objPts ):\r\n    def set_aspect_equal_3d(ax):\r\n        lims=np.array(ax.get_w_lims()).reshape((3, 2))\r\n        mid=np.mean(lims,axis=1)\r\n        halfWidth=np.abs(lims[:,0] - mid).max()\r\n        ax.set_xlim3d([mid[0] - halfWidth, mid[0] + halfWidth])\r\n        ax.set_ylim3d([mid[1] - halfWidth, mid[1] + halfWidth])\r\n        ax.set_zlim3d([mid[2] - halfWidth, mid[2] + halfWidth])\r\n\r\n    shortNames = oriental.utils.filePaths.ShortFileNames(list(projectionCenters))\r\n    ax = plt.figure('Object Space', clear=True).add_subplot(111, projection='3d')\r\n    ax.set_proj_type('ortho')\r\n    ax.plot([0, 10], [0,  0], [0,  0], 'r')\r\n    ax.plot([0,  0], [0, 10], [0,  0], 'g')\r\n    ax.plot([0,  0], [0,  0], [0, 10], 'b')\r\n\r\n    ax.set_xlabel('X')\r\n    ax.set_ylabel('Y')\r\n    ax.set_zlabel('Z')\r\n    ax.set_title('Object space ' + titleSuffix)\r\n    c = [ 'm' if name.startswith('a') else 'c' for name in objPts ]\r\n    ax.scatter( *zip(*objPts.values()), s=20, c=c)\r\n    for name, objPt in objPts.items():\r\n        if not name.startswith('a'):\r\n            ax.text(*objPt, name)\r\n    for name, projectionCenter in projectionCenters.items():\r\n        ax.text(*projectionCenter, shortNames(name))\r\n        R = alzeka2R( rotationAngles[name] )\r\n        for idx, color in enumerate('r g b'.split()):\r\n            endPt = [ 0, 0, 0 ]\r\n            endPt[idx] = 2\r\n            seg = np.array([ projectionCenter,\r\n                             R @ endPt + projectionCenter ])\r\n            ax.plot( seg[:,0], seg[:,1], seg[:,2], color=color )\r\n    set_aspect_equal_3d(ax)\r\n    plt.show(block=False)\r\n\r\n\r\ndef bundleBlock():\r\n    photo_obs = loadImageObservations()\r\n\r\n    # Deactivate points.\r\n    # This may be necessary if their intersection angles are small and image orientations are still imprecise.\r\n    # Once better estimates of interior and exterior image orientations are available, they may be activated, again.\r\n    pointsToIgnore = ['17', '19', '08', '20', '21', '22', '52'] #'28 29 30 31 32'.split()\r\n    for obs in photo_obs.values():\r\n        for pointName in pointsToIgnore:\r\n            obs.pop(pointName, None)\r\n\r\n    \"\"\" Define initial parameter values. \"\"\"\r\n\r\n    # TODO initial parameter values of the interior orientation\r\n    # Hopefully(!) the photos have been taken shortly one after another with the same camera,\r\n    # the same lens, and unchanged settings.\r\n    # Hence, use the same parameters of interior orientation for all photos.\r\n    # We need the coordinates of the projection center in the image coordinate system i.e. in [pixel].\r\n    # For the principal point, the image center is usually a good approximation.\r\n    # For the focal length in [pixel], we extract the focal length equivalent to 35mm film [mm]\r\n    # from the Exif image meta data e.g. using IrfanView.\r\n    # For the Samsung Galaxy S7's built-in camera, having taken the photos with the minimum focal length\r\n    # and having stored full resolution images, we consider the following values:\r\n    # - sensor / image resolution:\r\n    #   4032 x 3024 [px] (columns x rows)\r\n    # - nominal focal length equivalent to 35mm film:\r\n    #   26[mm]\r\n    # The nominal image area of 35mm film is: 36 x 24mm (width x height)\r\n\r\n    # 27mm 35 mm equivalent --> 3.95 mm\r\n    # Sensor Pixel Size\t1.22 Âµm = 0.00122 mm\r\n    #\r\n    ior = np.array([1500.,  # x_0\r\n                   -2000.,  # y_0\r\n                    4960.   # c [px]\r\n                    ], float)\r\n\r\n    ior = np.array([+1.49177e+03, -1.98762e+03, +4.45595e+03])\r\n\r\n    # TODO Distortion parameters.\r\n    # Choose an appropriate model.\r\n    # The number of distortion parameters stated here must match the number of distortion parameters that\r\n    # `PerspectiveCamera` expects. The preliminary number given here only serves as a placeholder.\r\n    distortion = np.array([+8.96235e-09, -3.00019e-15, +3.01027e-22, -4.24624e-07, -2.13253e-07])\r\n\r\n    # np.array([-1.05810e-08, +7.43013e-16, -1.94212e-07, +1.40238e-07])\r\n    # [1.025e-09 1.390e-16 3.008e-07 2.789e-07]\r\n    #distortion = np.array([0., 0., +105.633,  -56.806,   +2.371,   +2.721])\r\n\r\n    # `objPts` contains for each point name the object point coordinates as a vector with 3 elements.\r\n    objPts = dict()\r\n\r\n    # TODO Datum points.\r\n    # Wanted datum definition:\r\n    # - Unconstrained\r\n    # - the +Z axis of the object coordinate system shall point to the zenith, approximately, and\r\n    # - a coordinate plane of the object coordinate system shall be approximately\r\n    #   parallel to the frame of the window out of which the photos were taken.\r\n    # For an unconstrained datum definition, the 7 unknowns of a spatial similarity transform\r\n    # can be fixed e.g. by setting 7 object point coordinates constant.\r\n    # The distance observed with the measuring tape must be considered here.\r\n    # Introduce object point coordinates in units of meters.\r\n    objPts['01'] = np.array([0., 0., 1.2525], float)\r\n    objPts['03'] = np.array([0.501, 0., 0.], float)\r\n    objPts['04'] = np.array([0, 0., 0], float)\r\n\r\n    # TODO change to False once initial orientation is calculated\r\n    if False:\r\n        # At the very beginning, let's derive the exterior image orientations via spatial resection.\r\n        # For that purpose, we need object coordinates of at least 4 points manually observed in each image.\r\n        # Since the datum points do not suffice, define additional object coordinates here.\r\n        objPts['02'] = np.array([0.501, 0., 1.2555], float)\r\n        projectionCenters, rotationAngles = spatialResection(photo_obs, ior, objPts)\r\n    else:\r\n        # Once approximate exterior image orientations have been computed, copy them below and execute this branch.\r\n\r\n        # `projectionCenters` contains for each photo the coordinates of its projection center in the object CS.\r\n        # photoName :  np.array([X, Y, Z])\r\n        # projectionCenters = {\r\n        #     r'20200411_150033.jpg': np.array([+1.532, +0.329, -0.649]),\r\n        #     r'20200411_150100.jpg': np.array([+1.146, +0.156, -0.229]),\r\n        #     r'20200411_150233.jpg': np.array([+1.249, +0.271, -1.020]),\r\n        # }\r\n\r\n        projectionCenters = {\r\n            r'1.jpg': np.array([+2.36506e-01, -1.26043e+00, +9.40541e-01]),\r\n            r'2.jpg': np.array([+1.48141e-01, -1.67852e+00, +6.02067e-01]),\r\n            r'3.jpg': np.array([+6.89515e-01, -1.70342e+00, +5.81075e-01]),\r\n            r'4.jpg': np.array([+6.39841e-01, -1.49087e+00, +8.11608e-01])\r\n        }\r\n\r\n        # `rotationAngles` contains for each photo the rotation angles.\r\n        # `PerspectiveCamera` expects them in units of [gon], parameterized for alpha-zeta-kappa.\r\n        # That is, a rotation about\r\n        # 1. the z-axis\r\n        # 2. the y-axis\r\n        # 3. the z-axis.\r\n\r\n        # photoName : np.array([alpha, zeta, kappa])\r\n        # rotationAngles = {\r\n        #     r'20200411_150033.jpg': np.array([+11.822, +96.323, +98.453]),\r\n        #     r'20200411_150100.jpg': np.array([+16.517, +83.882, +3.914]),\r\n        #     r'20200411_150233.jpg': np.array([+22.009, +112.667, +6.396]),\r\n        # }\r\n\r\n        rotationAngles = {\r\n            r'1.jpg': np.array([-1.00062e+02, +8.69583e+01, +9.91641e+01]),\r\n            r'2.jpg': np.array([-1.06962e+02, +9.90409e+01, -7.60140e-01]),\r\n            r'3.jpg': np.array([-9.09183e+01, +9.99431e+01, -1.98898e+02]),\r\n            r'4.jpg': np.array([-8.53713e+01, +9.16614e+01, -9.87502e+01])\r\n        }\r\n\r\n    # Using the initial values of interior and exterior orientations,\r\n    # derive object point coordinates by spatial intersection.\r\n    initializeObjectCoordinates(photo_obs, projectionCenters, rotationAngles, ior, distortion, objPts)\r\n\r\n    print(\"Initial parameter values\")\r\n    printParameters(projectionCenters, rotationAngles, ior, distortion, objPts)\r\n\r\n    \"\"\" Define the bundle block adjustment \"\"\"\r\n    # The adjustment shall minimize the squared sum of residuals (L2-norm).\r\n    # Hence, use the 'trivial' loss-function, which simply computes the sum of squared residuals for each residual block.\r\n    loss = oriental.adjust.loss.Trivial()\r\n\r\n    # The central object for the definition and solution of adjustment problems:\r\n    block = oriental.adjust.Problem()\r\n\r\n    # Iterate over all photos.\r\n    for photo, observations in photo_obs.items():\r\n        # Iterate over all image observations of the current photo.\r\n        for ptName, imageCoordinates in observations.items():\r\n            x, y = imageCoordinates\r\n            # Instantiate the observation class defined above with the current image coordinates, the point and photo names.\r\n            cost = PerspectiveCamera(x, y, ptName, photo)\r\n\r\n            # Add a residual block for the current image observation to the adjustment problem.\r\n            block.AddResidualBlock(cost,\r\n                                   loss,\r\n                                   projectionCenters[photo],\r\n                                   rotationAngles[photo],\r\n                                   ior,\r\n                                   distortion,\r\n                                   objPts[ptName])\r\n\r\n    if False:\r\n        # TODO Once a good enough interior image orientation is known,\r\n        # automatically compute additional tie points.\r\n        featurePoints, featurePointMatches = getAutomaticTiePoints(list(photo_obs.keys()))\r\n        # `featurePoints` is a `dict` that maps each image file path to an array of feature point image coordinates.\r\n        # `featurePointMatches` is a `dict` that maps each image pair to an array of matching feature points.\r\n        # Each row of such an array contains in the first column an index into the feature point coordinates of the\r\n        # first photo, and likewise in the second column for the second photo.\r\n\r\n        imageObsPerObjPt = linkAutomaticTiePoints(featurePointMatches)\r\n        # Optionally, reduce the number of automatic tie points.\r\n        #imageObsPerObjPt = thinOutTracks(featurePoints, imageObsPerObjPt, nCols=4, nRows=3, minFeaturesPerCell=20)\r\n        # Add the additional image observations to the adjustment problem.\r\n        nAutoTieObjPts = 0\r\n        nAutoTieImgPts = 0\r\n        for imageObs in imageObsPerObjPt:\r\n            # Among the automatically extracted and matched feature points, outliers are to be expected,\r\n            # which must not be introduced into the adjustment.\r\n            # In order to detect outliers, intersect the object point and check the according image residuals.\r\n            # NOTE: since bundle adjustment has not been executed yet, these image residuals are based on the\r\n            # not yet adjusted image orientations!\r\n            # Hence, their initial values must be sufficiently accurate in order to reliably detect outliers here!\r\n\r\n            if len(imageObs) < 3:\r\n                continue # Drop feature matches in only 2 photos, since their redundancy is low and hence, outlier residuals may be small.\r\n\r\n            observations = []\r\n            for photo, idxImgPt in imageObs:\r\n                imageCoordinates = featurePoints[photo][idxImgPt]\r\n                observations.append((photo, imageCoordinates))\r\n\r\n            objPt, residuals = forwardIntersectionGeometric(observations, projectionCenters, rotationAngles, ior, distortion, maxLinResNorm=30)\r\n            if not np.isfinite(residuals).all():\r\n                # Object point triangulation failed.\r\n                continue\r\n            # If residuals are large, then the probability of this feature match being an outlier is high.\r\n            # TODO But how large is *large*?\r\n            maxResidualNorm = 300\r\n            residuals = np.sum(residuals**2, axis=1)**0.5\r\n            if residuals.max() > maxResidualNorm:\r\n                pass\r\n                # continue\r\n            # Another way to detect outliers is e.g. to check for extreme object point coordinates.\r\n\r\n            # Create a name for the automatically determined tie point by prefixing the index with an 'a'.\r\n            # Hopefully, this naming schema has not been used for manual tie points already.\r\n            ptName = f'a{nAutoTieObjPts:02}'\r\n            assert ptName not in objPts, f\"Name for automatically determined tie point already in use: {ptName}\"\r\n            nAutoTieObjPts += 1\r\n            # Add the tie point to the list of object points.\r\n            objPts[ptName] = objPt\r\n\r\n            for photo, idxImgPt in imageObs:\r\n                x, y = featurePoints[photo][idxImgPt]\r\n                cost = PerspectiveCamera(x, y, ptName, photo)\r\n                # For each image point, add a residual block to the adjustment problem.\r\n                block.AddResidualBlock(cost,\r\n                                       loss,\r\n                                       projectionCenters[photo],\r\n                                       rotationAngles[photo],\r\n                                       ior,\r\n                                       distortion,\r\n                                       objPt)\r\n                nAutoTieImgPts += 1\r\n\r\n        print(f\"Number of inserted automatic tie points: {nAutoTieImgPts} image points, {nAutoTieObjPts} object points\")\r\n\r\n    \"\"\" A priori residuals \"\"\"\r\n    residuals_apriori, = block.Evaluate()\r\n    print(\"Sum of squared errors: {}\".format(residuals_apriori @ residuals_apriori))\r\n    residualStatistics(block, residuals_apriori, k=10)\r\n\r\n    # # Pass show=True to display the plots in windows, pass show=False to save memory.\r\n    residualPlots(block, residuals_apriori, ior, 'a priori', show=False, scale=1.)\r\n    plot3d('a priori', projectionCenters, rotationAngles, objPts)\r\n\r\n    \"\"\" Datum definition \"\"\"\r\n\r\n    # Define the parameter blocks for which all elements shall be kept constant.\r\n    # The A-matrix queried below will not contain columns for these parameter blocks.\r\n    idsConstantBlocks = [id(el) for el in (objPts['03'],\r\n                                           objPts['04'],\r\n                                           # ior,  # TODO Set ior constant?\r\n                                           # distortion  # TODO Set distortion constant?\r\n                                          )]\r\n\r\n    # Since the unconstrained datum definition shall be accomplished by setting constant 7 object point coordinates,\r\n    # the third datum point must not be set constant as a whole.\r\n    # Instead, only set constant 1 of its coordinates to eliminate 7 unknowns in total.\r\n    # In order to only set constant a subset of the elements of a parameter block,\r\n    # use class `oriental.adjust.local_parameterization.Subset`.\r\n    # Its arguments are:\r\n    # 1. the size of the parameter block (which is 3 for object points), and\r\n    # 2. a vector of indices of the elements to be set constant.\r\n    # Indexing starts at 0, as usual. Hence, e.g. index 0 sets the X-coordinate constant.\r\n\r\n    subset = oriental.adjust.local_parameterization.Subset(3, np.array([1]))\r\n    block.SetParameterization(objPts['01'], subset)\r\n\r\n    # Set specific distortion parameters constant?\r\n    #subset = oriental.adjust.local_parameterization.Subset(distortion.size, np.array([0, 1]))\r\n    #block.SetParameterization(distortion, subset)\r\n\r\n    \"\"\" Bundle block adjustment \"\"\"\r\n    # Create a list of all parameter blocks.\r\n    # For printing results later on, also integrate into this list the name of each block.\r\n    # Hence, each element of this list is a pair consisting of a name and the respective parameter block.\r\n    paramBlockNamesAndValues = (\r\n          [(\"IOR\", ior)]\r\n        + [(\"Distortion\", distortion)]\r\n        + [(f\"PRC {name}\", value) for name, value in sorted(projectionCenters.items())]\r\n        + [(f\"ROT {name}\", value) for name, value in sorted(rotationAngles.items())]\r\n        + [(f\"OBJ {name}\", value) for name, value in sorted(objPts.items(), key=lambda x: sortPoints(x[0]))]\r\n    )\r\n    # Based on `paramBlockNamesAndValues` and `idsConstantBlocks`, create a list of non-constant parameter blocks.\r\n    variableBlocks = [value for name, value in paramBlockNamesAndValues if id(value) not in idsConstantBlocks]\r\n\r\n    evaluateOptions = oriental.adjust.Problem.EvaluateOptions()\r\n    # Pass the parameter blocks for which the A-matrix shall contain columns, in the wanted order.\r\n    evaluateOptions.set_parameter_blocks(variableBlocks)\r\n\r\n    # Iteration loop. Defines a maximum number of iterations, but should be terminated before:\r\n    for iIter in range(40):\r\n        print(\"------ Iteration: {}\".format(iIter))\r\n        # l consists of the concatenated results of `PerspectiveCamera.Evaluate`\r\n        #   i.e. observed image position minus computed projection.\r\n        #   The order of its rows is the one in which residual blocks have been added using `AddResidualBlock`.\r\n        # A is the matrix of partial derivatives of `l` w.r.t. the unknowns.\r\n        #   The order of its columns is the one of `variableBlocks`.\r\n        #   The order of its rows is the same as for `l`.\r\n        l, A = block.Evaluate(evaluateOptions, residuals=True, jacobian=True)\r\n\r\n        # `PerspectiveCamera` computes the partial derivatives of `l`, and not of the function value.\r\n        # Hence, invert the signs of `A`. At the same time, convert the sparse into a dense matrix.\r\n        A = -A.toarray()\r\n        Atl = A.T @ l\r\n\r\n        N = A.T @ A\r\n\r\n        # N is symmetric and positive definite.\r\n        # Hence, use the efficient Cholesky-factorization for solving the equation system.\r\n        C = linalg.cholesky(N, lower=False)\r\n\r\n        # The vector of parameter supplements.\r\n        delta_x = linalg.cho_solve((C, False), Atl)\r\n\r\n        # Apply the parameter supplements to the parameter blocks.\r\n        index=0\r\n        for paramBlock in variableBlocks:\r\n            # Get the number of actually estimated elements for the current parameter block.\r\n            # This is simply the parameter block's size, unless a local parameterization has been assigned to the block.\r\n            localParameterization = block.GetParameterization(paramBlock)\r\n            if localParameterization is None:\r\n                # No local parameterization has been assigned to the current block.\r\n                # Hence, supplements have been computed for all its elements.\r\n                variableParameters = np.ones(len(paramBlock), bool)\r\n            else:\r\n                # A local parameterization has been assigned to this block. Hence, the A-matrix does not contain columns\r\n                # for all of its elements, and less supplements have been computed.\r\n                variableParameters = localParameterization.constancyMask == 0\r\n            numberOfVariableParameters = variableParameters.sum()\r\n            paramBlock[variableParameters] += delta_x[index : index + numberOfVariableParameters]\r\n            index += numberOfVariableParameters\r\n\r\n        residuals_aposteriori, = block.Evaluate()\r\n        print(\"Sum of squared errors: {}\".format(residuals_aposteriori @ residuals_aposteriori))\r\n        print(\"Termination criterium: {}\".format(np.abs((l.T @ l - residuals_aposteriori.T @ residuals_aposteriori) / (l.T @ l))))\r\n\r\n        # When shall the iteration loop be terminated?\r\n        # TODO Define an appropriate stopping criterion here and break the loop early as soon as it is met.\r\n        if np.abs((l.T @ l - residuals_aposteriori.T @ residuals_aposteriori) / (l.T @ l)) < 1e-15:\r\n            print('-->')\r\n            print(residuals_aposteriori.T @ residuals_aposteriori - l.T @ l)\r\n            print('Stopping criterion met! Iteration: {}'.format(iIter))\r\n            break\r\n    else:\r\n        print(\"Warning: maximum number of iterations reached!\")\r\n\r\n    \"\"\" Results \"\"\"\r\n    # Redundancy.\r\n    # 2 observations per image point.\r\n    numberOfObservations = 2 * block.NumResidualBlocks()\r\n    assert numberOfObservations == A.shape[0], 'The number of observations does not match the number of rows of the A-matrix'\r\n\r\n    numberOfUnknowns = sum(block.ParameterBlockLocalSize(paramBlock) for paramBlock in variableBlocks)\r\n    assert numberOfUnknowns == A.shape[1], \"The number of unknowns defined by `variableBlocks` and the local parameterizations does not match the number of columns of the A-matrix\"\r\n\r\n    redundancy = numberOfObservations - numberOfUnknowns\r\n\r\n    residuals_aposteriori, = block.Evaluate()\r\n\r\n    print(\"Adjusted parameters\")\r\n    printParameters(projectionCenters, rotationAngles, ior, distortion, objPts)\r\n    residualStatistics(block, residuals_aposteriori, k=10)\r\n    # Pass show=True to display the plots in windows, pass show=False to save memory.\r\n    residualPlots(block, residuals_aposteriori, ior, 'a posteriori', show=False, scale=50.)\r\n    plot3d('a posteriori', projectionCenters, rotationAngles, objPts)\r\n\r\n    sumOfSquaredErrors = residuals_aposteriori @ residuals_aposteriori\r\n\r\n    sigma0 = (sumOfSquaredErrors / redundancy)**.5\r\n    print(f\"sigma0: {sigma0:.3}px\")\r\n\r\n    # Cofactor matrix of Unknowns\r\n    # Like in the iteration loop, compute the Cholesky factorization of the N-matrix.\r\n    # This time, however, solve for the identity matrix. Hence, the result is the inverse of N.\r\n    A, = block.Evaluate(evaluateOptions, residuals=False, jacobian=True)\r\n    A = -A.toarray()\r\n    N = A.T @ A\r\n    C = linalg.cholesky(N, lower=False)\r\n    Qxx = linalg.cho_solve((C,False), np.eye(C.shape[0]))\r\n\r\n    print(\"Standard deviations of unknowns\")\r\n    idxQxx=0\r\n    for paramBlockName, paramBlockValue in paramBlockNamesAndValues:\r\n        qxxDiag = np.zeros(len(paramBlockValue))\r\n        if id(paramBlockValue) in idsConstantBlocks:\r\n            continue\r\n        localParameterization = block.GetParameterization(paramBlockValue)\r\n        if localParameterization is None:\r\n            # No local parameterization has been assigned to the current block.\r\n            variableParameters = np.ones(len(paramBlockValue), bool)\r\n        else:\r\n            variableParameters = localParameterization.constancyMask == 0\r\n        for idxElem, isVariable in enumerate(variableParameters):\r\n            if isVariable:\r\n                qxxDiag[idxElem] = Qxx[idxQxx, idxQxx]\r\n                idxQxx += 1\r\n\r\n        stdDev = sigma0 * qxxDiag**0.5\r\n        print(\"{}: {} (indices: {}:{} )\".format(paramBlockName, np.array2string(stdDev, precision=3), idxQxx - variableParameters.sum(), idxQxx))\r\n\r\n    # TODO Check the significance, correlations, etc. of the parameters of the interior orientation and lens distortion.\r\n\r\n\r\n    # TODO Derive the wanted distances at the object and their precisions.\r\n    # ---------------------- 03 - 13 -----------------------------------------------------------------------------------\r\n    # coor_diff = objPts['03'] - objPts['13']\r\n    # s_03_13 = np.sqrt(np.sum(coor_diff**2))\r\n    #\r\n    # s = s_03_13\r\n    # A = np.array([[coor_diff[0] / s, coor_diff[1] / s, coor_diff[2] / s, -coor_diff[0] / s, -coor_diff[1] / s,\r\n    #               -coor_diff[2] / s]])\r\n    #\r\n    # C_xx_3 = sigma0 ** 2 * Qxx[34:37, 34:37]\r\n    # C_xx_13 = sigma0**2 * Qxx[61:64, 61:64]\r\n    #\r\n    # C_xx_3_13 = sigma0**2 * Qxx[34:37, 61:64]\r\n    # C_xx_13_3 = sigma0 ** 2 * Qxx[61:64, 34:37]\r\n    #\r\n    # C_xx_comb = np.vstack((\r\n    #     np.hstack((C_xx_3, C_xx_3_13)),\r\n    #     np.hstack((C_xx_13_3, C_xx_13))\r\n    # ))\r\n    #\r\n    # std_s_03_13 = np.sqrt(A @ C_xx_comb @ A.T)\r\n    # print(\"Strecke 03-13: {}\".format(s_03_13))\r\n    # print(\"STD Strecke 03-13: {}\".format(std_s_03_13))\r\n    #\r\n    # # ---------------------- 40 - 12 -----------------------------------------------------------------------------------\r\n    # coor_diff = objPts['12'] - objPts['40']\r\n    # s_12_40 = np.sqrt(np.sum(coor_diff ** 2))\r\n    #\r\n    # s = s_12_40\r\n    # A = np.array([[coor_diff[0] / s, coor_diff[1] / s, coor_diff[2] / s, -coor_diff[0] / s, -coor_diff[1] / s,\r\n    #                -coor_diff[2] / s]])\r\n    #\r\n    # C_xx_12 = sigma0 ** 2 * Qxx[58:61, 58:61]\r\n    # C_xx_40 = sigma0 ** 2 * Qxx[124:127, 124:127]\r\n    #\r\n    # C_xx_12_40 = sigma0 ** 2 * Qxx[58:61, 124:127]\r\n    # C_xx_40_12 = sigma0 ** 2 * Qxx[124:127, 58:61]\r\n    #\r\n    # C_xx_comb = np.vstack((\r\n    #     np.hstack((C_xx_12, C_xx_12_40)),\r\n    #     np.hstack((C_xx_40_12, C_xx_40))\r\n    # ))\r\n    #\r\n    # std_s_12_40 = np.sqrt(A @ C_xx_comb @ A.T)\r\n    # print(\"Strecke 12-40: {}\".format(s_12_40))\r\n    # print(\"STD Strecke 12-40: {}\".format(std_s_12_40))\r\n\r\n\r\nif __name__ == '__main__':\r\n    bundleBlock()\r\n    input('Hit key to exit')\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>windows-1252
===================================================================
--- python/buendelblock_2.py	(revision 4dac94cd217f5b297f5df1ce2fa412759295d7a1)
+++ python/buendelblock_2.py	(date 1598605101522)
@@ -62,7 +62,7 @@
 
 # The file path of the data base of image measurements done in MonoScope.
 # Assumes that it lies in the directory of this script, and that the default file name is used.
-dataBasePath = Path(os.path.abspath(__file__)).parent /'..\data2\MonoScope.sqlite'
+dataBasePath = Path(os.path.abspath(__file__)).parent /'\data2\MonoScope.sqlite'
 # dataBasePath = Path(r'..\data\picked_pts\MonoScope.sqlite')
 print("Path for the monoscope DB file: {}".format(dataBasePath))
 # Make the directory of the data base file the current working directory
@@ -771,8 +771,8 @@
     # 27mm 35 mm equivalent --> 3.95 mm
     # Sensor Pixel Size	1.22 Âµm = 0.00122 mm
     #
-    ior = np.array([1500.,  # x_0
-                   -2000.,  # y_0
+    ior = np.array([1488.,  # x_0
+                   -1984.,  # y_0
                     4960.   # c [px]
                     ], float)
 
@@ -806,7 +806,7 @@
     objPts['04'] = np.array([0, 0., 0], float)
 
     # TODO change to False once initial orientation is calculated
-    if False:
+    if True:
         # At the very beginning, let's derive the exterior image orientations via spatial resection.
         # For that purpose, we need object coordinates of at least 4 points manually observed in each image.
         # Since the datum points do not suffice, define additional object coordinates here.
@@ -965,8 +965,8 @@
     # The A-matrix queried below will not contain columns for these parameter blocks.
     idsConstantBlocks = [id(el) for el in (objPts['03'],
                                            objPts['04'],
-                                           # ior,  # TODO Set ior constant?
-                                           # distortion  # TODO Set distortion constant?
+                                            ior,  # TODO Set ior constant?
+                                            distortion  # TODO Set distortion constant?
                                           )]
 
     # Since the unconstrained datum definition shall be accomplished by setting constant 7 object point coordinates,
Index: plot_distortion.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nsensor_size = (6000, 4000)\r\n\r\nx = np.arange(-sensor_size[0]/2, sensor_size[0]/2, 1)\r\ny = np.arange(-sensor_size[1]/2, sensor_size[1]/2, 1)\r\n\r\nxx, yy = np.meshgrid(x, y)\r\nrho = (xx**2 + yy**2) ** (1/2)\r\n\r\nrho0 = (3000 ** 2 + 2000 ** 2) ** (1 / 2)\r\n\r\nk1, k2, k3, p1, p2 = (+4.38471e-09, +4.01227e-16, -4.26485e-23, -1.02340e-06, -4.16125e-07)\r\n\r\ndelta_x_dist = xx * (k1 * (rho - rho0) ** 2 + k2 * (rho - rho0) ** 4 + k3 * (rho - rho0) ** 6) + \\\r\n               2 * p1 * xx * yy + p2 * (rho ** 2 + 2 * xx ** 2)\r\ndelta_y_dist = yy * (k1 * (rho - rho0) ** 2 + k2 * (rho - rho0) ** 4 + k3 * (rho - rho0) ** 6) + \\\r\n               + p1 * (rho ** 2 + 2 * yy ** 2) + 2 * p2 * xx * yy\r\n\r\n# plt.matshow(delta_x_dist, cmap=\"PRGn\")\r\n# plt.colorbar()\r\n#\r\n# plt.matshow(delta_y_dist, cmap=\"PRGn\")\r\n# plt.colorbar()\r\n\r\ntotal_correction = (delta_x_dist ** 2 + delta_y_dist ** 2) ** (1/2)\r\nplt.matshow(total_correction)\r\ncb = plt.colorbar()\r\ncb.set_label(\"Betrag der Verzeichnungskorrektur [px]\")\r\n\r\n# plt.figure()\r\n# plt.quiver(xx, yy, delta_x_dist, delta_y_dist)\r\n\r\nplt.show()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- plot_distortion.py	(revision 4dac94cd217f5b297f5df1ce2fa412759295d7a1)
+++ plot_distortion.py	(date 1598604494157)
@@ -1,7 +1,7 @@
 import numpy as np
 import matplotlib.pyplot as plt
 
-sensor_size = (6000, 4000)
+sensor_size = (3968, 2976)
 
 x = np.arange(-sensor_size[0]/2, sensor_size[0]/2, 1)
 y = np.arange(-sensor_size[1]/2, sensor_size[1]/2, 1)
@@ -9,7 +9,7 @@
 xx, yy = np.meshgrid(x, y)
 rho = (xx**2 + yy**2) ** (1/2)
 
-rho0 = (3000 ** 2 + 2000 ** 2) ** (1 / 2)
+rho0 = (1984 ** 2 + 1488 ** 2) ** (1 / 2)
 
 k1, k2, k3, p1, p2 = (+4.38471e-09, +4.01227e-16, -4.26485e-23, -1.02340e-06, -4.16125e-07)
 
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.8 (PhotoVertiefungUe)\" project-jdk-type=\"Python SDK\" />\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/misc.xml	(revision 4dac94cd217f5b297f5df1ce2fa412759295d7a1)
+++ .idea/misc.xml	(date 1598601639625)
@@ -1,4 +1,4 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8 (PhotoVertiefungUe)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
Index: .idea/PhotoVertiefungUe.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\" />\r\n    <orderEntry type=\"jdk\" jdkName=\"Python 3.8 (PhotoVertiefungUe)\" jdkType=\"Python SDK\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/PhotoVertiefungUe.iml	(revision 4dac94cd217f5b297f5df1ce2fa412759295d7a1)
+++ .idea/PhotoVertiefungUe.iml	(date 1598601639615)
@@ -2,7 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="jdk" jdkName="Python 3.8 (PhotoVertiefungUe)" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Python 3.8" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
 </module>
\ No newline at end of file
diff --git .idea/runConfigurations/buendelblock.xml .idea/runConfigurations/buendelblock.xml
